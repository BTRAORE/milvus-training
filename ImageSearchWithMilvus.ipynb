{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QtpqkqhnhXy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Milvus Image Search**\n",
        "This notebook demonstrates how to perform image similarity search and text-to-image search using Milvus and the CLIP model.\n",
        "\n",
        "**Session Goal**: **To show how to store image embeddings in Milvus and perform visual similarity searches and cross-modal (text-to-image) searches.**\n",
        "\n",
        "## **Requirements**\n",
        "\n",
        "\n",
        "*   A milvus instance setup and running (ex: MilvusLite, Standalone Milvus, ...)\n",
        "*   Image embedding model, like CLIP model\n",
        "*   Images to be stored in Milvus\n",
        "*   Search images\n",
        "\n"
      ],
      "metadata": {
        "id": "EbGdlxvYn3Ml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Libraries and Set Up Milvus Lite\n"
      ],
      "metadata": {
        "id": "MEYfALPWsLtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymilvus[standalone_milvus] sentence-transformers matplotlib pillow==9.5.0 --quiet"
      ],
      "metadata": {
        "id": "_6HkB7LlsVoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image and text embedding Model:\n",
        "In our test we are going to use a CLIP embedding model:  **clip-ViT-B-32**"
      ],
      "metadata": {
        "id": "RFx153LMqcJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CLIP Model\n",
        "print(\"Loading CLIP model (this may take a moment)...\")\n",
        "# 'clip-ViT-B-32' is a good balance of performance and speed\n",
        "model = SentenceTransformer('clip-ViT-B-32')\n",
        "print(\"CLIP model loaded successfully.\")"
      ],
      "metadata": {
        "id": "7ycCeWSWrbSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generate Embeddings and Ingest them**\n",
        "Now we will generate embeddings for our images using ***clip-ViT-B-32*** model and insert the embeddings into Milvus"
      ],
      "metadata": {
        "id": "fDgPjnK1tfMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating embeddings and inserting into Milvus...\")\n",
        "milvus_entities = []\n",
        "for item in tqdm(image_data, desc=\"Encoding & Ingesting\"):\n",
        "    # Encode the image\n",
        "    img_embedding = model.encode(item[\"image_obj\"]).tolist() # Convert numpy array to list\n",
        "    milvus_entities.append({\n",
        "        \"image_id\": item[\"id\"],\n",
        "        \"image_path\": item[\"path\"],\n",
        "        \"embedding\": img_embedding\n",
        "    })\n",
        "\n",
        "# Insert entities in batches (MilvusClient handles batching automatically)\n",
        "client.insert(collection_name=COLLECTION_NAME, data=milvus_entities)\n",
        "\n",
        "# Ensure data is indexed and available for search\n",
        "client.flush(collection_name=COLLECTION_NAME)\n",
        "client.load_collection(collection_name=COLLECTION_NAME)\n",
        "\n",
        "print(f\"Successfully ingested {len(milvus_entities)} image embeddings into Milvus.\")\n",
        "print(f\"Collection entities count: {client.get_collection_stats(COLLECTION_NAME)['row_count']}\")"
      ],
      "metadata": {
        "id": "xNTw6TFLt_Gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Perform Image Search (Image-to-Image)**\n",
        "Here we pick an image and ask milvus to find similar ones."
      ],
      "metadata": {
        "id": "V6Poo-5YwyZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_item # ... selected image\n",
        "\n",
        "if query_item is None:\n",
        "    print(f\"Error: Image with ID {query_image_id} not found.\")\n",
        "else:\n",
        "    query_image_path = query_item['path']\n",
        "    query_image_obj = query_item['image_obj']\n",
        "    query_vector = model.encode(query_image_obj).tolist()\n",
        "\n",
        "    print(f\"Querying with image (ID: {query_image_id}):\")\n",
        "    display_images([query_image_path], titles=[\"Query Image\"], fig_size=(3,3))\n",
        "\n",
        "    # Perform the search\n",
        "    print(\"\\nSearching for similar images...\")\n",
        "    search_results = client.search(\n",
        "        collection_name=COLLECTION_NAME,\n",
        "        data=[query_vector],\n",
        "        anns_field=\"embedding\",\n",
        "        param={\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}},\n",
        "        limit=5, # Get top 5 results\n",
        "        output_fields=[\"image_path\", \"image_id\"]\n",
        "    )\n",
        "\n",
        "    # Process and display results\n",
        "    retrieved_image_paths = []\n",
        "    retrieved_titles = []\n",
        "    print(\"\\nSearch Results (Image-to-Image):\")\n",
        "    for hit in search_results[0]: # Loop through results for the first query\n",
        "        if hit.id != query_image_id: # Exclude the query image itself from results\n",
        "            retrieved_image_paths.append(hit[\"image_path\"])\n",
        "            retrieved_titles.append(f\"ID: {hit.id}, Dist: {hit.distance:.4f}\")\n",
        "\n",
        "    display_images(retrieved_image_paths, titles=retrieved_titles, fig_size=(15, 4))"
      ],
      "metadata": {
        "id": "uYA67r2oyZkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Perform Text Search (Text-to-Image)**\n",
        "Thanks to CLIP, we can also search for images using a text description!"
      ],
      "metadata": {
        "id": "WlbcYQU6y-IV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your text query\n",
        "text_query = \"a furry animal in the snow\" # Try changing this!\n",
        "# text_query = \"a person skiing\"\n",
        "# text_query = \"a bridge over water\"\n",
        "# text_query = \"a big grey animal\"\n",
        "\n",
        "# Generate embedding for the text query\n",
        "query_text_vector = model.encode(text_query).tolist()\n",
        "\n",
        "print(f\"Querying with text: '{text_query}'\")\n",
        "\n",
        "# Perform the search\n",
        "print(\"\\nSearching for images based on text query...\")\n",
        "search_results = client.search(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    data=[query_text_vector],\n",
        "    anns_field=\"embedding\",\n",
        "    param={\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}},\n",
        "    limit=5, # Get top 5 results\n",
        "    output_fields=[\"image_path\", \"image_id\"]\n",
        ")\n",
        "\n",
        "# Process and display results\n",
        "retrieved_image_paths = []\n",
        "retrieved_titles = []\n",
        "print(\"\\nSearch Results (Text-to-Image):\")\n",
        "for hit in search_results[0]:\n",
        "    retrieved_image_paths.append(hit[\"image_path\"])\n",
        "    retrieved_titles.append(f\"ID: {hit.id}, Dist: {hit.distance:.4f}\")\n",
        "\n",
        "display_images(retrieved_image_paths, titles=retrieved_titles, fig_size=(15, 4)"
      ],
      "metadata": {
        "id": "UYv7VMF0znUg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}